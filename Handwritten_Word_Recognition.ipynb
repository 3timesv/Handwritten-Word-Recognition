{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwritten_Word_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_RLZkkihux6S",
        "f1CcRM9wrq7c",
        "-LGob8z_stVh",
        "fMH48nv-s_YI",
        "oL6VreD3tjIJ",
        "RdBFUMzCt0o5",
        "GjeEHLkquWqt",
        "RqhD2VPIur6s",
        "7TFyMV0qxCJI",
        "JgnSq-24y0ly"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekverma1019/Handwritten-Word-Recognition/blob/master/Handwritten_Word_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RLZkkihux6S",
        "colab_type": "text"
      },
      "source": [
        "###### import libraries and mount Gdrive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbwZWrSx0KNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import random \n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK3-9o-am7C0",
        "colab_type": "code",
        "outputId": "18109d0b-ca1e-4203-dcdd-93de9cd6edf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1CcRM9wrq7c",
        "colab_type": "text"
      },
      "source": [
        "###### readfile() -->returns  (dict = {filename : gt}) given .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjwnIl6XgIsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readfile(filename):\n",
        "    res = []\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for i in lines:\n",
        "            res.append(i.strip())\n",
        "        f.close()\n",
        "    dic = {}\n",
        "    for i in res:\n",
        "        p = i.split(' ')\n",
        "        dic[p[0]] = p[-1]\n",
        "    return dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGob8z_stVh",
        "colab_type": "text"
      },
      "source": [
        "###### split words.txt into train.txt and test.txt with 0.95 : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCmXRHmHSMJV",
        "colab_type": "code",
        "outputId": "74fd7214-00c2-4e9c-a4fd-a096ab7ff6f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "trn = full[:int(len(full)*0.95)]\n",
        "tst = full[int(len(full)*0.95):]\n",
        "full = []\n",
        "with open('/content/drive/My Drive/Downloads/iam_words/words.txt','r') as f:\n",
        "    lines = f.readlines()\n",
        "    lines = lines[18:]\n",
        "    for i in lines:\n",
        "        full.append(i.strip('\\n'))\n",
        "tr = open('/content/drive/My Drive/Downloads/iam_words/train.txt', 'w+')\n",
        "for i in trn:\n",
        "    tr.write(i + '\\n')\n",
        "tr.close()\n",
        "ts = open('/content/drive/My Drive/Downloads/iam_words/test.txt', 'w+')\n",
        "for i in tst:\n",
        "    ts.write(i + '\\n')\n",
        "ts.close()\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrn = full[:int(len(full)*0.95)]\\ntst = full[int(len(full)*0.95):]\\nfull = []\\nwith open('/content/drive/My Drive/Downloads/iam_words/words.txt','r') as f:\\n    lines = f.readlines()\\n    lines = lines[18:]\\n    for i in lines:\\n        full.append(i.strip('\\n'))\\ntr = open('/content/drive/My Drive/Downloads/iam_words/train.txt', 'w+')\\nfor i in trn:\\n    tr.write(i + '\\n')\\ntr.close()\\nts = open('/content/drive/My Drive/Downloads/iam_words/test.txt', 'w+')\\nfor i in tst:\\n    ts.write(i + '\\n')\\nts.close()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMH48nv-s_YI",
        "colab_type": "text"
      },
      "source": [
        "###### random_uniform_num():: get() : return shuffled indexes given batchsize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cgv3SZjZdsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class random_uniform_num():\n",
        "    def __init__(self, total):\n",
        "        self.total = total\n",
        "        self.range = [i for i in range(total)]\n",
        "        np.random.shuffle(self.range)\n",
        "        self.index = 0\n",
        "    def get(self, batchsize):\n",
        "        r_n=[]\n",
        "        if(self.index + batchsize > self.total):\n",
        "            r_n_1 = self.range[self.index:self.total]\n",
        "            np.random.shuffle(self.range)\n",
        "            self.index = (self.index + batchsize) - self.total\n",
        "            r_n_2 = self.range[0:self.index]\n",
        "            r_n.extend(r_n_1)\n",
        "            r_n.extend(r_n_2)\n",
        "        else:\n",
        "            r_n = self.range[self.index : self.index + batchsize]\n",
        "            self.index = self.index + batchsize\n",
        "        return r_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL6VreD3tjIJ",
        "colab_type": "text"
      },
      "source": [
        "###### define charList = [chars] and char_dic={char : count}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfkiB5PgTwN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charList = []\n",
        "chars = \" !\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "for i in range(len(chars)):\n",
        "    charList.append(chars[i])\n",
        "char_dic = {}\n",
        "for i,j in enumerate(charList):\n",
        "    char_dic[j] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdBFUMzCt0o5",
        "colab_type": "text"
      },
      "source": [
        "###### preprocess() : returns preprocessed image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG-5KJ5em6Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(img, imgSize, dataAugmentation=False):\n",
        "\tif img is None:\n",
        "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
        "\tif dataAugmentation:\n",
        "\t\tstretch = (random.random() - 0.5) \n",
        "\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1)\n",
        "\t\timg = cv2.resize(img, (wStretched, img.shape[0]))\n",
        "\t(wt, ht) = imgSize\n",
        "\t(h, w) = img.shape\n",
        "\tfx = w / wt\n",
        "\tfy = h / ht\n",
        "\tf = max(fx, fy)\n",
        "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "\timg = cv2.resize(img, newSize)\n",
        "\ttarget = np.ones([ht, wt]) * 255\n",
        "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
        "\timg = cv2.transpose(target)\n",
        "\t(m, s) = cv2.meanStdDev(img)\n",
        "\tm = m[0][0]\n",
        "\ts = s[0][0]\n",
        "\timg = img - m\n",
        "\timg = img / s if s>0 else img\n",
        "\treturn img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjeEHLkquWqt",
        "colab_type": "text"
      },
      "source": [
        "###### gen() : mini-batch generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXzm6fRlMV4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen(data_file, image_path, batchsize=50, maxlabellength=32, imagesize=(128, 32)):\n",
        "    image_label = readfile(data_file)\n",
        "    _imagefile = [i for i, j in image_label.items()]\n",
        "    x = np.zeros((batchsize,1, imagesize[0], imagesize[1]), dtype=np.float)\n",
        "    labels = np.ones([batchsize, maxlabellength]) * 10000\n",
        "    input_length = np.zeros([batchsize, 1])\n",
        "    label_length = np.zeros([batchsize, 1])\n",
        "    r_n = random_uniform_num(len(_imagefile))\n",
        "    _imagefile = np.array(_imagefile)\n",
        "    while 1:\n",
        "        shufimagefile = _imagefile[r_n.get(batchsize)]\n",
        "        for i, j in enumerate(shufimagefile):\n",
        "            img1 = cv2.imread(os.path.join(image_path, j.split('-')[0] , '-'.join([j.split('-')[0],j.split('-')[1]]) ,j+'.png'),cv2.IMREAD_GRAYSCALE)\n",
        "            img = preprocess(img1 , imagesize)\n",
        "            x[i] = np.expand_dims(img, axis=0)\n",
        "            str = image_label[j]\n",
        "            if len(str) == 53:\n",
        "                str = str[:32]\n",
        "            label_length[i] = len(str)\n",
        "            if(len(str) <= 0):\n",
        "                print(\"len < 0\", j)\n",
        "            input_length[i] = 32\n",
        "            labels[i, :len(str)] = [char_dic[k] for k in str]\n",
        "\n",
        "        inputs = {'the_input': x,\n",
        "                'the_labels': labels,\n",
        "                'input_length': input_length,\n",
        "                'label_length': label_length,\n",
        "                }\n",
        "        outputs = {'ctc': np.zeros([batchsize])}\n",
        "        yield (inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqhD2VPIur6s",
        "colab_type": "text"
      },
      "source": [
        "###### define basemodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da3Auqrg3UFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.Input(shape=(1,128,32),name='the_input')\n",
        "\n",
        "l1_cn = layers.Conv2D(filters=32,kernel_size=(5,5),strides=(1,1),data_format='channels_first',padding='same')(inputs)\n",
        "l1_bn = layers.BatchNormalization(axis=1)(l1_cn)\n",
        "l1_a = layers.Activation('relu')(l1_bn)\n",
        "l1_mp = layers.MaxPooling2D(pool_size=(2,2) , padding=\"valid\",data_format=\"channels_first\")(l1_a)\n",
        "\n",
        "l2_cn = layers.Conv2D(filters=64,kernel_size=(5,5),strides=(1,1),data_format=\"channels_first\",padding='same')(l1_mp)\n",
        "l2_bn = layers.BatchNormalization(axis=1)(l2_cn)\n",
        "l2_a = layers.Activation('relu')(l2_bn)\n",
        "l2_mp = layers.MaxPooling2D(pool_size=(2,2) , padding=\"valid\",data_format=\"channels_first\")(l2_a)\n",
        "\n",
        "l3_cn = layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),data_format=\"channels_first\",padding='same')(l2_mp)\n",
        "l3_bn = layers.BatchNormalization(axis=1)(l3_cn)\n",
        "l3_a = layers.Activation('relu')(l3_bn)\n",
        "l3_mp = layers.MaxPooling2D(pool_size=(1,2) , padding=\"valid\",data_format=\"channels_first\")(l3_a)\n",
        "\n",
        "l4_cn = layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),data_format=\"channels_first\",padding='same')(l3_mp)\n",
        "l4_bn = layers.BatchNormalization(axis=1)(l4_cn)\n",
        "l4_a = layers.Activation('relu')(l4_bn)\n",
        "l4_mp = layers.MaxPooling2D(pool_size=(1,2) , padding=\"valid\",data_format=\"channels_first\")(l4_a)\n",
        "\n",
        "l5_cn = layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),data_format=\"channels_first\",padding='same')(l4_mp)\n",
        "l5_bn = layers.BatchNormalization(axis=1)(l5_cn)\n",
        "l5_a = layers.Activation('relu')(l5_bn)\n",
        "l5_mp = layers.MaxPooling2D(pool_size=(1,2) , padding=\"valid\",data_format=\"channels_first\")(l5_a)\n",
        "\n",
        "r_s = layers.Permute((2,1))(tf.squeeze(l5_mp,axis=3))\n",
        "l6_bl = layers.Bidirectional(layer=layers.LSTM(units=256,return_sequences=True), merge_mode='concat')(r_s)\n",
        "\n",
        "l7_bl = layers.Bidirectional(layer=layers.LSTM(units=256,return_sequences=True), merge_mode='concat')(l6_bl)\n",
        "\n",
        "l8_cn = layers.Conv2D(filters = 80,kernel_size=(1,1),data_format='channels_last',padding='same',kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1))(tf.expand_dims(l7_bl,2))\n",
        "l8_sf = layers.Activation(\"softmax\")(l8_cn)\n",
        "y_pred = tf.squeeze(l8_sf,axis=2)\n",
        "\n",
        "basemodel = tf.keras.Model(inputs = inputs,outputs = y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TFyMV0qxCJI",
        "colab_type": "text"
      },
      "source": [
        "###### define ctc lambda function for ctc loss calculation\n",
        "###### define model\n",
        "###### compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0J194KIvJ_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "labels = tf.keras.Input(name='the_labels', shape=[None], dtype='float32')\n",
        "input_length = tf.keras.Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = tf.keras.Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "loss_out = tf.keras.layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
        "model = tf.keras.Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
        "\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgnSq-24y0ly",
        "colab_type": "text"
      },
      "source": [
        "###### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjX9hS1qTYAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "maxlabellength = 32\n",
        "imagesize = (128,32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXcLXUxuWZIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = gen('/content/drive/My Drive/Downloads/iam_words/train.txt', '/content/drive/My Drive/Downloads/iam_words/words', batchsize=batch_size, maxlabellength=maxlabellength, imagesize=imagesize)\n",
        "test_loader = gen('/content/drive/My Drive/Downloads/iam_words/test.txt', '/content/drive/My Drive/Downloads/iam_words/words', batchsize=batch_size, maxlabellength=maxlabellength, imagesize=imagesize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9ukZVlRk_o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/models/weights-{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP-BGW7FWYyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(train_loader,\n",
        "    \tsteps_per_epoch = 109554//batch_size,\n",
        "    \tepochs = 10,\n",
        "    \tinitial_epoch = 0,\n",
        "    \tvalidation_data = test_loader,\n",
        "    \tvalidation_steps = 5766//batch_size,\n",
        "        callbacks = [checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}